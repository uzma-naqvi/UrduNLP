#Task: BiLSTM-SA Urdu Text

#Mount google drive for data
from google.colab import drive
drive.mount('/drive')

path='/drive/My Drive/SA/'


def get_embeddings(emb_name):
 from gensim.models import KeyedVectors
 import gensim
 emb={}
 mtype={}
 if emb_name=='samar':
     model = KeyedVectors.load_word2vec_format(path+'urduvec_140M_100K_300d.bin', binary=True)
     mtype="wd2v"
     emb_dim=300
      
 emb=model
 return emb,mtype,emb_dim

import nltk
import os
import re
#Preprocessing of data
with open(path+'urdu_stopwords.txt', encoding="UTF-8-sig", errors='ígnore') as f:
        sw  = f.readlines()
sw= [x.strip() for x in sw]

def tokenize(filetext):
    file_sentences = []
    for sentence in nltk.sent_tokenize(filetext):
       text = re.sub(r"\d+", " ", sentence)
       # English punctuations
       text = re.sub(r"""[!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~]+""", " ", text)
       # Urdu punctuations
       text = re.sub(r"[:؛؟’‘٭ء،۔]+", " ", text)
       # Arabic numbers
       text = re.sub(r"[٠‎١‎٢‎٣‎٤‎٥‎٦‎٧‎٨‎٩]+", " ", text)
       text = re.sub(r"[^\w\s]", " ", text)
       # Remove English characters and numbers.
       text = re.sub(r"[a-zA-z0-9]+", " ", text)
       # remove multiple spaces.
       text = re.sub(r" +", " ", text)
       text = text.split(" ")
       # some stupid empty tokens should be removed.
       text = [t.strip() for t in text if t.strip()]
       result=[]
       for t in text:
         for stp in range((len(sw))-1): 
#               if re.match(t,sw[i]):
               if t==str(sw[stp]):  
                    t=''
#               else:
#                 i+=1 
#                 match=0 
         if t.strip() != '':
          result.append(t)         
    text=result
    file_sentences.extend(text)
    return file_sentences

# -*- coding: utf-8 -*-
"""
Created on Fri Jan  8 14:11:28 2021

@author: Administrator
"""

import nltk
nltk.download("punkt")
import os
from gensim.models import KeyedVectors
import gensim
# import fasttext
import re
import pandas as pd
import numpy as np
from numpy import zeros

#from keras.utils import plot_model
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from keras.layers import Dense,GlobalMaxPooling1D,Dropout
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input,Embedding,Bidirectional, LSTM,concatenate 
from keras.callbacks import EarlyStopping, ModelCheckpoint
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score,confusion_matrix,auc, precision_score, recall_score, f1_score, cohen_kappa_score,roc_auc_score,roc_curve
from keras.regularizers import l2

class Attention(tf.keras.Model):
    def __init__(self, units):
        super(Attention, self).__init__()
        self.W1 = tf.keras.layers.Dense(units)
        self.W2 = tf.keras.layers.Dense(units)
        self.V = tf.keras.layers.Dense(1)

    def call(self, features, hidden):
       # we are doing this to perform addition to calculate the score
        hidden_with_time_axis = tf.expand_dims(hidden, 1)
          
        # score shape == (batch_size, max_length, 1)
        # we get 1 at the last axis because we are applying score to self.V
        # the shape of the tensor before applying self.V is (batch_size, max_length, units)
        score = tf.nn.tanh(
            self.W1(features) + self.W2(hidden_with_time_axis))
        # attention_weights shape == (batch_size, max_length, 1)
        attention_weights = tf.nn.softmax(self.V(score), axis=1)
          
        # context_vector shape after sum == (batch_size, hidden_size)
        context_vector = attention_weights * features
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector, attention_weights
vocab_size = 120000
max_length = 150
trunc_type='post'
padding_type='post'
oov_tok = "<OOV>"

sentences=[]
labels=[]

df = pd.read_csv(path+'Urdutrainingdatasetlabeled(301021).csv',header=None)
#df = df.sample(frac = 1)
for i in range(len(df)) : 
    sentences.append(tokenize(df.iloc[i, 0]))
    labels.append(df.iloc[i, 1])


y = np.array(list(map(lambda x: 1 if x=="p" else 0, labels)))    

tokenizer = Tokenizer(vocab_size,oov_token=oov_tok)
X_train, X_test,Y_train, Y_test = train_test_split(sentences, y, test_size=0.2, random_state = 1)
tokenizer.fit_on_texts(sentences)
words_to_index = tokenizer.word_index
vocab_size = len(words_to_index)+1
max_words=max([len(x) for x in sentences])
i=0
# creating training sequences and padding them
train_sequences=tokenizer.texts_to_sequences(X_train)
training_padded=pad_sequences(train_sequences,maxlen = max_words, padding='post', truncating = 'post')

# creating  testing sequences and padding them using same tokenizer
testing_sequences = tokenizer.texts_to_sequences(X_test)
testing_padded = pad_sequences(testing_sequences,maxlen = max_words,
                                padding = 'post', truncating='post')
input_shape=(training_padded.shape)    

# converting all variables to numpy arrays
training_padded = np.array(training_padded)
training_labels = np.array(Y_train)
testing_padded = np.array(testing_padded)
testing_labels = np.array(Y_test)


embeddings_index={}
embeddings_index_samar={}

embeddings_index_Samar,m_type,emb_dim_Samar=get_embeddings("samar")
embedding_matrix_Samar = zeros((vocab_size, emb_dim_Samar))
for word, i in tokenizer.word_index.items():
  if m_type=="wd2v":
    if word in embeddings_index_Samar:
        embedding_matrix_Samar[i] = embeddings_index_Samar[word]





embedding_layer_samar = Embedding(vocab_size,
                            emb_dim_Samar,
                            weights=[embedding_matrix_Samar],
                            input_length=max_words,
                            trainable=False)      

sequence_input = Input(shape=(max_words,), dtype='int32')

embedded_sequences_samar = embedding_layer_samar(sequence_input)    
lstm1=Bidirectional(LSTM(128, return_sequences = True,recurrent_dropout=0.2,dropout=0.5,kernel_regularizer=l2(1e-6)), name="bi_lstm_0")(embedded_sequences_samar)
(lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(128, return_sequences=True, return_state=True), name="bi_lstm_1")(lstm1)
state_h = concatenate([forward_h, backward_h])
state_c = concatenate([forward_c, backward_c])
context_vector, attention_weights = Attention(15)(lstm, state_h)
dense1 = Dense(20, activation="relu")(context_vector)
dropout = Dropout(0.05)(dense1)
output = Dense(1, activation="sigmoid")(dropout)
  
Model_Samar = tf.keras.Model(inputs=sequence_input, outputs=output)
Model_Samar.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
Model_Samar.summary()
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3,restore_best_weights = True)  
Model_Samar.fit(training_padded, training_labels, epochs=15,validation_data=(testing_padded, testing_labels),callbacks=[es])
#loss, accuracy = Model_Samar.evaluate(testing_padded, testing_labels,verbose=False)
yhat_probs_Samar = Model_Samar.predict(testing_padded, verbose=0)

# reduce to 1d array
yhat_probs_Samar = yhat_probs_Samar[:, 0]
#yhat_classes_Samar = yhat_classes[:, 0]
#labels = [1, 0]
yhat_classes_Samar=[]
for prob in yhat_probs_Samar:
    if prob>0.5: 
        yhat_classes_Samar.append(1)
    else:  
        yhat_classes_Samar.append(0)       

# # accuracy: (tp + tn) / (p + n)
accuracy_Samar = accuracy_score(testing_labels, yhat_classes_Samar)
print('Accuracy: %f' % accuracy_Samar)


precision_Samar = precision_score(testing_labels, yhat_classes_Samar)
print('Precision: %f' % precision_Samar)

# # recall: tp / (tp + fn)
recall_Samar = recall_score(testing_labels, yhat_classes_Samar)
print('Recall: %f' % recall_Samar)

f1_Samar = f1_score(testing_labels, yhat_classes_Samar)
print('F1 score: %f' % f1_Samar)


roc_Samar = roc_auc_score(testing_labels, yhat_probs_Samar)
print('ROC AUC: %f' % roc_Samar)

# # confusion matrix
matrix_Samar = confusion_matrix(testing_labels, yhat_classes_Samar)


y_pred_keras_Samar =Model_Samar.predict(testing_padded).ravel()


fpr_keras_Samar, tpr_keras_Samar, thresholds_keras_Samar = roc_curve(testing_labels, y_pred_keras_Samar)
auc_keras_Samar = auc(fpr_keras_Samar, tpr_keras_Samar)


# plt.figure(1)
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr_keras_Samar, tpr_keras_Samar, label='Samar (area = {:.3f})'.format(auc_keras_Samar))

plt.legend(loc='best')
plt.show()

import seaborn as sns
 #Samar Confusion Matrix
group_names = ['True Neg','False Pos','False Neg','True Pos']
group_counts = ["{0:0.0f}".format(value) for value in
                 matrix_Samar.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in
                      matrix_Samar.flatten()/np.sum(matrix_Samar)]
labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
           zip(group_names,group_counts,group_percentages)]
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(matrix_Samar, annot=labels, fmt='', cmap='Blues')
